@article{Barbieri2001,
abstract = {A paradigm for constructing and analyzing non-Poisson stimulus-response models of neural spike train activity is presented. Inhomogeneous gamma (IG) and inverse Gaussian (IIG) probability models are constructed by generalizing the derivation of the inhomogeneous Poisson (IP) model from the exponential probability density. The resultant spike train models have Markov dependence. Quantile-quantile (Q-Q) plots and Kolmogorov-Smirnov (K-S) plots are developed based on the rate-rescaling theorem to assess model goodness-of-fit. The analysis also expresses the spike rate function of the neuron directly in terms of its interspike interval (ISI) distribution. The methods are illustrated with an analysis of 34 spike trains from rat CA1 hippocampal pyramidal neurons recorded while the animal executed a behavioral task. The stimulus in these experiments is the animal's position in its environment and the response is the neural spiking activity. For all 34 pyramidal cells, the IG and IIG models gave better fits to the spike trains than the IP. The IG model more accurately described the frequency of longer ISIs, whereas the IIG model gave the best description of the burst frequency, i.e. ISIs≤20 ms. The findings suggest that bursts are a significant component of place cell spiking activity even when position and the background variable, theta phase, are taken into account. Unlike the Poisson model, the spatial and temporal rate maps of the IG and IIG models depend directly on the spiking history of the neurons. These rate maps are more physiologically plausible since the interaction between space and time determines local spiking propensity. While this statistical paradigm is being developed to study information encoding by rat hippocampal neurons, the framework should be applicable to stimulus-response experiments performed in other neural systems. Copyright {\textcopyright} 2001 Elsevier Science B.V.},
author = {Barbieri, Riccardo and Quirk, Michael C and Frank, Loren M and Wilson, Matthew A and Brown, Emery N},
doi = {10.1016/S0165-0270(00)00344-7},
issn = {01650270},
journal = {Journal of Neuroscience Methods},
keywords = {Hippocampal place cells,Inhomogeneous Poisson process,Inhomogeneous gamma process,Inhomogeneous inverse Gaussian process,Interspike interval distributions,Kolmogorov-Smirnov plots,Markov models,Quantile-quantile plots,Rate-rescaling theorem},
month = {jan},
number = {1},
pages = {25--37},
pmid = {11166363},
title = {{Construction and analysis of non-Poisson stimulus-response models of neural spiking activity}},
volume = {105},
year = {2001}
}
@article{Ghanbari2019,
abstract = {Objective. Neural responses to repeated presentations of an identical stimulus often show substantial trial-To-Trial variability. How the mean firing rate varies in response to different stimuli or during different movements (tuning curves) has been extensively modeled in a wide variety of neural systems. However, the variability of neural responses can also have clear tuning independent of the tuning in the mean firing rate. This suggests that the variability could contain information regarding the stimulus/movement beyond what is encoded in the mean firing rate. Here we demonstrate how taking variability into account can improve neural decoding. Approach. In a typical neural coding model spike counts are assumed to be Poisson with the mean response depending on an external variable, such as a stimulus or movement. Bayesian decoding methods then use the probabilities under these Poisson tuning models (the likelihood) to estimate the probability of each stimulus given the spikes on a given trial (the posterior). However, under the Poisson model, spike count variability is always exactly equal to the mean (Fano factor = 1). Here we use two alternative models-the Conway-Maxwell-Poisson (CMP) model and negative binomial (NB) model-to more flexibly characterize how neural variability depends on external stimuli. These models both contain the Poisson distribution as a special case but have an additional parameter that allows the variance to be greater than the mean (Fano factor > 1) or, for the CMP model, less than the mean (Fano factor < 1). Main results. We find that neural responses in primary motor (M1), visual (V1), and auditory (A1) cortices have diverse tuning in both their mean firing rates and response variability. Across cortical areas, we find that Bayesian decoders using the CMP or NB models improve stimulus/movement estimation accuracy by 4%-12% compared to the Poisson model. Significance. Moreover, the uncertainty of the non-Poisson decoders more accurately reflects the magnitude of estimation errors. In addition to tuning curves that reflect average neural responses, stimulus-dependent response variability may be an important aspect of the neural code. Modeling this structure could, potentially, lead to improvements in brain machine interfaces.},
author = {Ghanbari, Abed and Lee, Christopher M. and Read, Heather L. and Stevenson, Ian H.},
doi = {10.1088/1741-2552/ab3a68},
issn = {17412552},
journal = {Journal of Neural Engineering},
keywords = {generalized linear model,neural coding,neural variability},
number = {6},
pmid = {31404915},
title = {{Modeling stimulus-dependent variability improves decoding of population neural responses}},
volume = {16},
year = {2019}
}
@article{Kelly2010,
abstract = {Multineuronal recordings have revealed that neurons in primary visual cortex (V1) exhibit coordinated fluctuations of spiking activity in the absence and in the presence of visual stimulation. From the perspective of understanding a single cell's spiking activity relative to a behavior or stimulus, these network fluctuations are typically considered to be noise. We show that these events are highly correlated with another commonly recorded signal, the local field potential (LFP), and are also likely related to global network state phenomena which have been observed in a number of neural systems. Moreover, we show that attributing a component of cell firing to these network fluctuations via explicit modeling of the LFP improves the recovery of cell properties. This suggests that the impact of network fluctuations may be estimated using the LFP, and that a portion of this network activity is unrelated to the stimulus and instead reflects ongoing cortical activity. Thus, the LFP acts as an easily accessible bridge between the network state and the spiking activity. {\textcopyright} Springer Science+Business Media, LLC 2010.},
author = {Kelly, Ryan C. and Smith, Matthew A. and Kass, Robert E. and Lee, Tai Sing},
doi = {10.1007/s10827-009-0208-9},
issn = {09295313},
journal = {Journal of Computational Neuroscience},
keywords = {Correlation,Decoding,Local field potential,Multielectrode array,Network state,Population coding,Spontaneous activity},
month = {dec},
number = {3},
pages = {567--579},
pmid = {20094906},
publisher = {J Comput Neurosci},
title = {{Local field potentials indicate network state and account for neuronal response variability}},
url = {https://pubmed.ncbi.nlm.nih.gov/20094906/},
volume = {29},
year = {2010}
}
@article{Rokni2007,
author = {Rokni, U and Richardson, A G and Bizzi, E and Seung, H S},
journal = {Neuron},
number = {4},
pages = {653--666},
title = {{Motor learning with unstable neural representations}},
volume = {54},
year = {2007}
}
@article{Churchland2010,
abstract = {Neural responses are typically characterized by computing the mean firing rate, but response variability can exist across trials. Many studies have examined the effect of a stimulus on the mean response, but few have examined the effect on response variability. We measured neural variability in 13 extracellularly recorded datasets and one intracellularly recorded dataset from seven areas spanning the four cortical lobes in monkeys and cats. In every case, stimulus onset caused a decline in neural variability. This occurred even when the stimulus produced little change in mean firing rate. The variability decline was observed in membrane potential recordings, in the spiking of individual neurons and in correlated spiking variability measured with implanted 96-electrode arrays. The variability decline was observed for all stimuli tested, regardless of whether the animal was awake, behaving or anaesthetized. This widespread variability decline suggests a rather general property of cortex, that its state is stabilized by an input. {\textcopyright} 2010 Nature America, Inc. All rights reserved.},
author = {Churchland, Mark M and Yu, Byron M and Cunningham, John P and Sugrue, Leo P and Cohen, Marlene R and Corrado, Greg S and Newsome, William T and Clark, Andrew M and Hosseini, Paymon and Scott, Benjamin B and Bradley, David C and Smith, Matthew a and Kohn, Adam and Movshon, J Anthony and Armstrong, Katherine M and Moore, Tirin and Chang, Steve W and Snyder, Lawrence H and Lisberger, Stephen G and Priebe, Nicholas J and Finn, Ian M and Ferster, David and Ryu, Stephen I and Santhanam, Gopal and Sahani, Maneesh and Shenoy, Krishna V},
doi = {10.1038/nn.2501},
isbn = {1546-1726 (Electronic)\n1097-6256 (Linking)},
issn = {10976256},
journal = {Nature Neuroscience},
number = {3},
pages = {369--378},
pmid = {20173745},
publisher = {Nature Publishing Group},
title = {{Stimulus onset quenches neural variability: A widespread cortical phenomenon}},
volume = {13},
year = {2010}
}

@article{Paninski2010,
author = {Paninski, Liam and Ahmadian, Yashar and Ferreira, Daniel Gil and Koyama, Shinsuke and Rahnama Rad, Kamiar and Vidne, Michael and Vogelstein, Joshua and Wu, Wei},
title = {A New Look at State-Space Models for Neural Data},
year = {2010},
issue_date = {August 2010},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {1–2},
issn = {0929-5313},
url = {https://doi.org/10.1007/s10827-009-0179-x},
doi = {10.1007/s10827-009-0179-x},
abstract = {State space methods have proven indispensable in neural data analysis. However, common methods for performing inference in state-space models with non-Gaussian observations rely on certain approximations which are not always accurate. Here we review direct optimization methods that avoid these approximations, but that nonetheless retain the computational efficiency of the approximate methods. We discuss a variety of examples, applying these direct optimization techniques to problems in spike train smoothing, stimulus decoding, parameter estimation, and inference of synaptic properties. Along the way, we point out connections to some related standard statistical methods, including spline smoothing and isotonic regression. Finally, we note that the computational methods reviewed here do not in fact depend on the state-space setting at all; instead, the key property we are exploiting involves the bandedness of certain matrices. We close by discussing some applications of this more general point of view, including Markov chain Monte Carlo methods for neural decoding and efficient estimation of spatially-varying firing rates.},
journal = {J. Comput. Neurosci.},
month = {aug},
pages = {107–126},
numpages = {20},
keywords = {Hidden Markov model, Tridiagonal matrix, Neural coding, State-space models}
}
@article{Steinmetz2021,
abstract = {Measuring the dynamics of neural processing across time scales requires following the spiking of thousands of individual neurons over milliseconds and months. To address this need, we introduce the Neuropixels 2.0 probe together with newly designed analysis algorithms. The probe has more than 5000 sites and is miniaturized to facilitate chronic implants in small mammals and recording during unrestrained behavior. High-quality recordings over long time scales were reliably obtained in mice and rats in six laboratories. Improved site density and arrangement combined with newly created data processing methods enable automatic post hoc correction for brain movements, allowing recording from the same neurons for more than 2 months. These probes and algorithms enable stable recordings from thousands of sites during free behavior, even in small animals such as mice.},
author = {Steinmetz, Nicholas A. and Aydin, Cagatay and Lebedeva, Anna and Okun, Michael and Pachitariu, Marius and Bauza, Marius and Beau, Maxime and Bhagat, Jai and B{\"{o}}hm, Claudia and Broux, Martijn and Chen, Susu and Colonell, Jennifer and Gardner, Richard J. and Karsh, Bill and Kloosterman, Fabian and Kostadinov, Dimitar and Mora-Lopez, Carolina and O'Callaghan, John and Park, Junchol and Putzeys, Jan and Sauerbrei, Britton and van Daal, Rik J.J. and Vollan, Abraham Z. and Wang, Shiwei and Welkenhuysen, Marleen and Ye, Zhiwen and Dudman, Joshua T. and Dutta, Barundeb and Hantman, Adam W. and Harris, Kenneth D. and Lee, Albert K. and Moser, Edvard I. and O'Keefe, John and Renart, Alfonso and Svoboda, Karel and H{\"{a}}usser, Michael and Haesler, Sebastian and Carandini, Matteo and Harris, Timothy D.},
doi = {10.1126/science.abf4588},
issn = {10959203},
journal = {Science},
month = {apr},
number = {6539},
pmid = {33859006},
publisher = {American Association for the Advancement of Science},
title = {{Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings}},
volume = {372},
year = {2021}
}
@article{Lesica2007,
abstract = {In this study, we characterize the adaptation of neurons in the cat lateral geniculate nucleus to changes in stimulus contrast and correlations. By comparing responses to high- and low-contrast natural scene movie and white noise stimuli, we show that an increase in contrast or correlations results in receptive fields with faster temporal dynamics and stronger antagonistic surrounds, as well as decreases in gain and selectivity. We also observe contrast- and correlation-induced changes in the reliability and sparseness of neural responses. We find that reliability is determined primarily by processing in the receptive field (the effective contrast of the stimulus), while sparseness is determined by the interactions between several functional properties. These results reveal a number of adaptive phenomena and suggest that adaptation to stimulus contrast and correlations may play an important role in visual coding in a dynamic natural environment. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
author = {Lesica, Nicholas A. and Jin, Jianzhong and Weng, Chong and Yeh, Chun I. and Butts, Daniel A. and Stanley, Garrett B. and Alonso, Jose Manuel},
doi = {10.1016/j.neuron.2007.07.013},
isbn = {0896-6273 (Print)\r0896-6273 (Linking)},
issn = {08966273},
journal = {Neuron},
keywords = {SYSNEURO},
number = {3},
pages = {479--491},
pmid = {17678859},
title = {{Adaptation to Stimulus Contrast and Correlations during Natural Visual Stimulation}},
volume = {55},
year = {2007}
}
@article{Chestek2007,
abstract = {Some movements that animals and humans make are highly stereotyped, repeated with little variation. The patterns of neural activity associated with repeats of a movement may be highly similar, or the same movement may arise from different patterns of neural activity, if the brain exploits redundancies in the neural projections to muscles. We examined the stability of the relationship between neural activity and behavior. We asked whether the variability in neural activity that we observed during repeated reaching was consistent with a noisy but stable relationship, or with a changing relationship, between neural activity and behavior. Monkeys performed highly similar reaches under tight behavioral control, while many neurons in the dorsal aspect of premotor cortex and the primary motor cortex were simultaneously monitored for several hours. Neural activity was predominantly stable over time in all measured properties: firing rate, directional tuning, and contribution to a decoding model that predicted kinematics from neural activity. The small changes in neural activity that we did observe could be accounted for primarily by subtle changes in behavior. We conclude that the relationship between neural activity and practiced behavior is reasonably stable, at least on timescales of minutes up to 48 h. This finding has significant implications for the design of neural prosthetic systems because it suggests that device recalibration need not be overly frequent, It also has implications for studies of neural plasticity because a stable baseline permits identification of nonstationary shifts. Copyright {\textcopyright} 2007 Society for Neuroscience.},
author = {Chestek, Cynthia A and Batista, Aaron P and Santhanam, Gopal and Yu, Byron M and Afshar, Afsheen and Cunningham, John P and Gilja, Vikash and Ryu, Stephen I and Churchland, Mark M and Shenoy, Krishna V},
doi = {10.1523/JNEUROSCI.0959-07.2007},
isbn = {1529-2401 (Electronic)},
issn = {02706474},
journal = {Journal of Neuroscience},
keywords = {Arm,Brain machine interface,Decoding,Macaque,Multielectrode array,Premotor},
language = {eng},
number = {40},
pages = {10742--10750},
pmid = {17913908},
title = {{Single-neuron stability during repeated reaching in macaque premotor cortex}},
volume = {27},
year = {2007}
}
@article{Lesica2007a,
abstract = {In this study, we characterize the adaptation of neurons in the cat lateral geniculate nucleus to changes in stimulus contrast and correlations. By comparing responses to high- and low-contrast natural scene movie and white noise stimuli, we show that an increase in contrast or correlations results in receptive fields with faster temporal dynamics and stronger antagonistic surrounds, as well as decreases in gain and selectivity. We also observe contrast- and correlation-induced changes in the reliability and sparseness of neural responses. We find that reliability is determined primarily by processing in the receptive field (the effective contrast of the stimulus), while sparseness is determined by the interactions between several functional properties. These results reveal a number of adaptive phenomena and suggest that adaptation to stimulus contrast and correlations may play an important role in visual coding in a dynamic natural environment. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
author = {Lesica, Nicholas A. and Jin, Jianzhong and Weng, Chong and Yeh, Chun I. and Butts, Daniel A. and Stanley, Garrett B. and Alonso, Jose Manuel},
doi = {10.1016/j.neuron.2007.07.013},
isbn = {0896-6273 (Print)\r0896-6273 (Linking)},
issn = {08966273},
journal = {Neuron},
keywords = {SYSNEURO},
number = {3},
pages = {479--491},
pmid = {17678859},
title = {{Adaptation to Stimulus Contrast and Correlations during Natural Visual Stimulation}},
volume = {55},
year = {2007}
}
@article{Stevenson2011a,
abstract = {In systems neuroscience, neural activity that represents movements or sensory stimuli is often characterized by spatial tuning curves that may change in response to training, attention, altered mechanics, or the passage of time. A vital step in determining whether tuning curves change is accounting for estimation uncertainty due to measurement noise. In this study, we address the issue of tuning curve stability using methods that take uncertainty directly into account. We analyze data recorded from neurons in primary motor cortex using chronically implanted, multielectrode arrays in four monkeys performing center-out reaching. With the use of simulations, we demonstrate that under typical experimental conditions, the effect of neuronal noise on estimated preferred direction can be quite large and is affected by both the amount of data and the modulation depth of the neurons. In experimental data, we find that after taking uncertainty into account using bootstrapping techniques, the majority of neurons appears to be very stable on a timescale of minutes to hours. Lastly, we introduce adaptive filtering methods to explicitly model dynamic tuning curves. In contrast to several previous findings suggesting that tuning curves may be in constant flux, we conclude that the neural representation of limb movement is, on average, quite stable and that impressions to the contrary may be largely the result of measurement noise. {\textcopyright} 2011 the American Physiological Society.},
author = {Stevenson, Ian H. and Cherian, Anil and London, Brian M. and Sachs, Nicholas A. and Lindberg, Eric and Reimer, Jacob and Slutzky, Marc W. and Hatsopoulos, Nicholas G. and Miller, Lee E. and Kording, Konrad P.},
doi = {10.1152/jn.00626.2010},
issn = {00223077},
journal = {Journal of Neurophysiology},
keywords = {Neurons,Poisson noise,Primary motor cortex,Sensory stimuli,Spatial tuning curves,Tuning curves},
month = {aug},
number = {2},
pages = {764--774},
pmid = {21613593},
publisher = {J Neurophysiol},
title = {{Statistical assessment of the stability of neural movement representations}},
url = {https://pubmed.ncbi.nlm.nih.gov/21613593/},
volume = {106},
year = {2011}
}
@article{Shmueli2005,
abstract = {A useful discrete distribution (the Conway-Maxwell-Poisson distribution) is revived and its statistical and probabilistic properties are introduced and explored. This distribution is a two-parameter extension of the Poisson distribution that generalizes some well-known discrete distributions (Poisson, Bernoulli and geometric). It also leads to the generalization of distributions derived from these discrete distributions (i.e. the binomial and negative binomial distributions). We describe three methods for estimating the parameters of the Conway-Maxwell-Poisson distribution. The first is a fast simple weighted least squares method, which leads to estimates that are sufficiently accurate for practical purposes. The second method, using maximum likelihood, can be used to refine the initial estimates. This method requires iterations and is more computationally intensive. The third estimation method is Bayesian. Using the conjugate prior, the posterior density of the parameters of the Conway-Maxwell-Poisson distribution is easily computed. It is a flexible distribution that can account for overdispersion or underdispersion that is commonly encountered in count data. We also explore two sets of real world data demonstrating the flexibility and elegance of the Conway-Maxwell-Poisson distribution in fitting count data which do not seem to follow the Poisson distribution.},
author = {Shmueli, Galit and Minka, Thomas P. and Kadane, Joseph B. and Borle, Sharad and Boatwright, Peter},
doi = {10.1111/J.1467-9876.2005.00474.X},
issn = {1467-9876},
journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
keywords = {Conjugate family,Conway,Estimation,Exponential family,Maxwell,Overdispersion,Poisson distribution,Underdispersion},
month = {jan},
number = {1},
pages = {127--142},
publisher = {John Wiley & Sons, Ltd},
title = {{A useful distribution for fitting discrete data: revival of the Conway–Maxwell–Poisson distribution}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9876.2005.00474.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9876.2005.00474.x https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2005.00474.x},
volume = {54},
year = {2005}
}
@article{Macke2011,
abstract = {Neurons in the neocortex code and compute as part of a locally interconnected population. Large-scale multi-electrode recording makes it possible to access these population processes empirically by fitting statistical models to unaveraged data. What statistical structure best describes the concurrent spiking of cells within a local network? We argue that in the cortex, where firing exhibits extensive correlations in both time and space and where a typical sample of neurons still reflects only a very small fraction of the local population, the most appropriate model captures shared variability by a low-dimensional latent process evolving with smooth dynamics, rather than by putative direct coupling. We test this claim by comparing a latent dynamical model with realistic spiking observations to coupled gen-eralised linear spike-response models (GLMs) using cortical recordings. We find that the latent dynamical approach outperforms the GLM in terms of goodness-of-fit, and reproduces the temporal correlations in the data more accurately. We also compare models whose observations models are either derived from a Gaussian or point-process models, finding that the non-Gaussian model provides slightly better goodness-of-fit and more realistic population spike counts.},
author = {Macke, Jakob H. and Buesing, Lars and Cunningham, John P. and Yu, Byron M. and Shenoy, Krishna V. and Sahani, Maneesh},
journal = {Advances in Neural Information Processing Systems},
title = {{Empirical models of spiking in neural populations}},
volume = {24},
year = {2011}
}
@article{Chatla2018,
abstract = {The Conway–Maxwell–Poisson (CMP) or COM–Poisson regression is a popular model for count data due to its ability to capture both under dispersion and over dispersion. However, CMP regression is limited when dealing with complex nonlinear relationships. With today's wide availability of count data, especially due to the growing collection of data on human and social behavior, there is need for count data models that can capture complex nonlinear relationships. One useful approach is additive models; but, there has been no additive model implementation for the CMP distribution. To fill this void, we first propose a flexible estimation framework for CMP regression based on iterative reweighed least squares (IRLS) and then extend this model to allow for additive components using a penalized splines approach. Because the CMP distribution belongs to the exponential family, convergence of IRLS is guaranteed under some regularity conditions. Further, it is also known that IRLS provides smaller standard errors compared to gradient-based methods. We illustrate the usefulness of this approach through extensive simulation studies and using real data from a bike sharing system in Washington, DC.},
author = {Chatla, Suneel Babu and Shmueli, Galit},
doi = {10.1016/J.CSDA.2017.11.011},
issn = {0167-9473},
journal = {Computational Statistics & Data Analysis},
keywords = {IRLS,Over and under dispersion,P-IRLS,Penalized splines,Time series},
month = {may},
pages = {71--88},
publisher = {North-Holland},
title = {{Efficient estimation of COM–Poisson regression and a generalized additive model}},
volume = {121},
year = {2018}
}
@article{Wei2021,
abstract = {Synapses change on multiple timescales, ranging from milliseconds to minutes, due to a combination of both short- and long-term plasticity. Here we develop an extension of the common generalized linear model to infer both short- and long-term changes in the coupling between a pre- and postsynaptic neuron based on observed spiking activity. We model short-term synaptic plasticity using additive effects that depend on the presynaptic spike timing, and we model long-term changes in both synaptic weight and baseline firing rate using point process adaptive smoothing. Using simulations, we first show that this model can accurately recover time-varying synaptic weights (1) for both depressing and facilitating synapses, (2) with a variety of long-term changes (including realistic changes, such as due to STDP), (3) with a range of pre and postsynaptic firing rates, and (4) for both excitatory and inhibitory synapses. We then apply our model to two experimentally recorded putative synaptic connections. We find that simultaneously tracking fast changes in synaptic weights, slow changes in synaptic weights, and unexplained variations in baseline firing is essential. Omitting any one of these factors can lead to spurious inferences for the others. Altogether, this model provides a flexible framework for tracking short- and long-term variation in spike transmission.},
archivePrefix = {arXiv},
arxivId = {2102.01803},
author = {Wei, Ganchao and Stevenson, Ian H.},
doi = {10.1162/NECO_A_01426},
eprint = {2102.01803},
issn = {1530-888X},
journal = {Neural computation},
keywords = {Ganchao Wei,Ian H Stevenson,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-P.H.S.,PubMed Abstract,Research Support,U.S. Gov't,doi:10.1162/neco_a_01426,pmid:34530452},
month = {sep},
number = {10},
pages = {2682--2709},
pmid = {34530452},
publisher = {Neural Comput},
title = {{Tracking Fast and Slow Changes in Synaptic Weights From Simultaneously Observed Pre- and Postsynaptic Spiking}},
url = {https://pubmed.ncbi.nlm.nih.gov/34530452/},
volume = {33},
year = {2021}
}
@article{Fenton1998,
abstract = {The idea that the rat hippocampus stores a map of space is based on the existence of 'place cells' that show 'location-specific' firing. The discharge of place cells is confined with remarkable precision to a cell- specific part of the environment called the cell's 'firing field.' We demonstrate here that firing is not nearly as reliable in the time domain as in the positional domain. Discharge during passes through the firing field was compared with a model with Poisson variance of the location-specific firing determined by the time-averaged positional firing rate distribution. Place cells characteristically fire too little or too much compared with expectations from the random model. This fundamental property of place cells is referred to as 'excess firing variance' and has three main implications: (i) Place cell discharge is not only driven by the summation of many small, asynchronous excitatory synaptic inputs. (ii) Place cell discharge may encode a signal in addition to the current head location. (iii) The excess firing variance helps explain why the errors in computing the rat's position from the simultaneous activity of many place cells are large.},
author = {Fenton, Andr{\'{e}} A. and Muller, Robert U.},
doi = {10.1073/pnas.95.6.3182},
issn = {00278424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {6},
pages = {3182--3187},
pmid = {9501237},
title = {{Place cell discharge is extremely variable during individual passes of the rat through the firing field}},
volume = {95},
year = {1998}
}
@article{Smith2008,
abstract = {The spiking activity of cortical neurons is correlated. For instance, trial-to-trial fluctuations in response strength are shared between neurons, and spikes often occur synchronously. Understanding the properties and mechanisms that generate these forms of correlation is critical for determining their role in cortical processing. We therefore investigated the spatial extent and functional specificity of correlated spontaneous and evoked activity. Because feedforward, recurrent, and feedback pathways have distinct extents and specificity, we reasoned that these measurements could elucidate the contribution of each type of input. We recorded single unit activity with microelectrode arrays which allowed us to measure correlation in many hundreds of pairings, across a large range of spatial scales. Our data show that correlated evoked activity is generated by two mechanisms that link neurons with similar orientation preferences on different spatial scales: one with high temporal precision and a limited spatial extent (∼3 mm), and a second that gives rise to correlation on a slow time scale and extends as far as we were able to measure (10 mm). The former is consistent with common input provided by horizontal connections; the latter likely involves feedback from extrastriate cortex. Spontaneous activity was correlated over a similar spatial extent, but approximately twice as strongly as evoked activity. Visual stimuli thus caused a substantial decrease in correlation, particularly at response onset. These properties and the circuit mechanism they imply provide new constraints on the functional role that correlation may play in visual processing. Copyright {\textcopyright} 2008 Society for Neuroscience.},
author = {Smith, Matthew A. and Kohn, Adam},
doi = {10.1523/JNEUROSCI.2929-08.2008},
issn = {02706474},
journal = {Journal of Neuroscience},
keywords = {Array,Cross-correlogram,Multielectrode recordings,Noise correlation,Population coding,Signal correlation,Spontaneous activity,Synchrony},
month = {nov},
number = {48},
pages = {12591--12603},
pmid = {19036953},
title = {{Spatial and temporal scales of neuronal correlation in primary visual cortex}},
volume = {28},
year = {2008}
}
@article{Churchland2011,
abstract = {Traditionally, insights into neural computation have been furnished by averaged firing rates from many stimulus repetitions or trials. We pursue an analysis of neural response variance to unveil neural computations that cannot be discerned from measures of average firing rate. We analyzed single-neuron recordings from the lateral intraparietal area (LIP), during a perceptual decision-making task. Spike count variance was divided into two components using the law of total variance for doubly stochastic processes: (1) variance of counts that would be produced by a stochastic point process with a given rate, and loosely (2) the variance of the rates that would produce those counts (i.e., " conditional expectation" ). The variance and correlation of the conditional expectation exposed several neural mechanisms: mixtures of firing rate states preceding the decision, accumulation of stochastic " evidence" during decision formation, and a stereotyped response at decision end. These analyses help to differentiate among several alternative decision-making models. {\textcopyright} 2011 Elsevier Inc.},
author = {Churchland, Anne K and Kiani, R. and Chaudhuri, R. and Wang, Xiao Jing and Pouget, Alexandre and Shadlen, M. N.},
doi = {10.1016/j.neuron.2010.12.037},
isbn = {1097-4199 (Electronic)\r0896-6273 (Linking)},
issn = {08966273},
journal = {Neuron},
number = {4},
pages = {818--831},
pmid = {21338889},
title = {{Variance as a Signature of Neural Computations during Decision Making}},
volume = {69},
year = {2011}
}
@article{Gupta2014,
abstract = {In this paper, we further study the Conway–Maxwell Poisson distribution having one more parameter than the Poisson distribution and compare it with the Poisson distribution with respect to some stochastic orderings used in reliability theory. Likelihood ratio test and the score test are developed to test the importance of this additional parameter. Simulation studies are carried out to examine the performance of the two tests. Two examples are presented, one showing overdispersion and the other showing underdispersion, to illustrate the procedure. It is shown that the COM-Poisson model fits better than the generalized Poisson distribution.},
author = {Gupta, Ramesh C. and Sim, S. Z. and Ong, S. H.},
doi = {10.1007/S10182-014-0226-4/TABLES/7},
issn = {1863818X},
journal = {AStA Advances in Statistical Analysis},
keywords = {Failure rate,Overdispersion,Score test,Stochastic comparisons,Underdispersion},
month = {oct},
number = {4},
pages = {327--343},
publisher = {Springer Verlag},
title = {{Analysis of discrete data by Conway–Maxwell Poisson distribution}},
url = {https://link.springer.com/article/10.1007/s10182-014-0226-4},
volume = {98},
year = {2014}
}
@article{Dickey2009,
abstract = {The use of chronic intracortical multielectrode arrays has become increasingly prevalent in neurophysiological experiments. However, it is not obvious whether neuronal signals obtained over multiple recording sessions come from the same or different neurons. Here, we develop a criterion to assess single-unit stability by measuring the similarity of 1) average spike waveforms and 2) interspike interval histograms (ISIHs). Neuronal activity was recorded from four Utah arrays implanted in primary motor and premotor cortices in three rhesus macaque monkeys during 10 recording sessions over a 15- to 17-day period. A unit was defined as stable through a given day if the stability criterion was satisfied on all recordings leading up to that day. We found that 57% of the original units were stable through 7 days, 43% were stable through 10 days, and 39% were stable through 15 days. Moreover, stable units were more likely to remain stable in subsequent recording sessions (i.e., 89% of the neurons that were stable through four sessions remained stable on the fifth). Using both waveform and ISIH data instead of just waveforms improved performance by reducing the number of false positives. We also demonstrate that this method can be used to track neurons across days, even during adaptation to a visuomotor rotation. Identifying a stable subset of neurons should allow the study of long-term learning effects across days and has practical implications for pooling of behavioral data across days and for increasing the effectiveness of brain-machine interfaces. Copyright {\textcopyright} 2009 The American Physiological Society.},
author = {Dickey, Adam S and Suminski, Aaron and Amit, Yali and Hatsopoulos, Nicholas G},
doi = {10.1152/jn.90920.2008},
isbn = {0022-3077},
issn = {00223077},
journal = {Journal of Neurophysiology},
number = {2},
pages = {1331--1339},
pmid = {19535480},
publisher = {Am Physiological Soc},
title = {{Single-unit stability using chronically implanted multielectrode arrays}},
volume = {102},
year = {2009}
}
@article{DeWeese1998,
abstract = {It has long been recognized that sensory systems adapt to their inputs. Here we formulate the problem of optimal variance estimation for a broad class of nonstationary signals. We show that under weak assumptions, the Bayesian optimal causal variance estimate shows asymmetric dynamics: an abrupt increase in variance is more readily detectable than an abrupt decrease. By contrast, optimal adaptation to the mean displays symmetric dynamics when the variance is held fixed. After providing several empirical examples and a simple intuitive argument for our main result, we prove that optimal adaptation is asymmetrical in a broad class of model environments. This observation makes specific and falsifiable predictions about the time course of adaptation in neurons probed with certain stimulus ensembles.},
author = {DeWeese, Michael and Zador, Anthony},
doi = {10.1162/089976698300017403},
issn = {08997667},
journal = {Neural Computation},
number = {5},
pages = {1179--1202},
publisher = {MIT Press},
title = {{Asymmetric Dynamics in Optimal Variance Adaptation}},
volume = {10},
year = {1998}
}
@article{Brown2001,
abstract = {Neural receptive fields are plastic: with experience, neurons in many brain regions change their spiking responses to relevant stimuli. Analysis of receptive field plasticity from experimental measurements is crucial for understanding how neural systems adapt their representations of relevant biological information. Current analysis methods using histogram estimates of spike rate functions in nonoverlapping temporal windows do not track the evolution of receptive field plasticity on a fine time scale. Adaptive signal processing is an established engineering paradigm for estimating time-varying system parameters from experimental measurements. We present an adaptive filter algorithm for tracking neural receptive field plasticity based on point process models of spike train activity. We derive an instantaneous steepest descent algorithm by using as the criterion function the instantaneous log likelihood of a point process spike train model. We apply the point process adaptive filter algorithm in a study of spatial (place) receptive field properties of simulated and actual spike train data from rat CA1 hippocampal neurons. A stability analysis of the algorithm is sketched in the [Appendix][1]. The adaptive algorithm can update the place field parameter estimates on a millisecond time scale. It reliably tracked the migration, changes in scale, and changes in maximum firing rate characteristic of hippocampal place fields in a rat running on a linear track. Point process adaptive filtering offers an analytic method for studying the dynamics of neural receptive fields.

 [1]: #app-1},
author = {Brown, Emery N. and Nguyen, David P. and Frank, Loren M. and Wilson, Matthew A. and Solo, Victor},
doi = {10.1073/PNAS.201409398},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
month = {oct},
number = {21},
pages = {12261--12266},
pmid = {11593043},
publisher = {National Academy of Sciences},
title = {{An analysis of neural receptive field plasticity by point process adaptive filtering}},
url = {https://www.pnas.org/content/98/21/12261},
volume = {98},
year = {2001}
}
@article{Sellers2010,
author = {Kimberly F. Sellers and Galit Shmueli},
title = {{A flexible regression model for count data}},
volume = {4},
journal = {The Annals of Applied Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {943 -- 961},
keywords = {Conway–Maxwell-Poisson (COM-Poisson) distribution, dispersion, generalized linear models (GLM), generalized Poisson},
year = {2010},
doi = {10.1214/09-AOAS306},
URL = {https://doi.org/10.1214/09-AOAS306}
}
@article{Stevenson2016,
abstract = {A key observation in systems neuroscience is that neural responses vary, even in controlled settings where stimuli are held constant. Many statistical models assume that trial-to-trial spike count variability is Poisson, but there is considerable evidence that neurons can be substantially more or less variable than Poisson depending on the stimuli, attentional state, and brain area. Here we examine a set of spike count models based on the Conway-Maxwell-Poisson (COM-Poisson) distribution that can flexibly account for both over- and under-dispersion in spike count data. We illustrate applications of this noise model for Bayesian estimation of tuning curves and peri-stimulus time histograms. We find that COM-Poisson models with group/observation-level dispersion, where spike count variability is a function of time or stimulus, produce more accurate descriptions of spike counts compared to Poisson models as well as negative-binomial models often used as alternatives. Since dispersion is one determinant of parameter standard errors, COM-Poisson models are also likely to yield more accurate model comparison. More generally, these methods provide a useful, model-based framework for inferring both the mean and variability of neural responses.},
author = {Stevenson, Ian H.},
doi = {10.1007/s10827-016-0603-y},
issn = {15736873},
journal = {Journal of Computational Neuroscience},
keywords = {Conway-Maxwell-Poisson,Poisson,Spike count variability,Tuning curves},
month = {aug},
number = {1},
pages = {29--43},
pmid = {27008191},
publisher = {Springer New York LLC},
title = {{Flexible models for spike count data with both over- and under- dispersion}},
url = {https://link.springer.com/article/10.1007/s10827-016-0603-y},
volume = {41},
year = {2016}
}
@misc{Kohn2016,
author = {Kohn, A. and Smith, M.A.},
booktitle = {CRCNS.org},
title = {{Utah array extracellular recordings of spontaneous and visually evoked activity from anesthetized macaque primary visual cortex (V1)}},
url = {http://dx.doi.org/10.6080/K0NC5Z4X},
year = {2016}
}
@article{Eden2004,
abstract = {Neural receptive fields are dynamic in that with experience, neurons change their spiking responses to relevant stimuli. To understand how neural systems adapt the irrepresentations of biological information, analyses of receptive field plasticity from experimental measurements are crucial. Adaptive signal processing, the well-established engineering discipline for characterizing the temporal evolution of system parameters, suggests a framework for studying the plasticity of receptive fields. We use the Bayes' rule Chapman-Kolmogorov paradigm with a linear state equation and point process observation models to derive adaptive filters appropriate for estimation from neural spike trains. We derive point process filter analogues of the Kalman filter, recursive least squares, and steepest-descent algorithms and describe the properties of these new fil-ters. We illustrate our algorithms in two simulated data examples. The first is a study of slow and rapid evolution of spatial receptive fields in hippocampal neurons. The second is an adaptive decoding study in which a signal is decoded from ensemble neural spiking activity as the recep-tive fields of the neurons in the ensemble evolve. Our results provide a paradigm for adaptive estimation for point process observations and suggest a practical approach for constructing filtering algorithms to track neural receptive field dynamics on a millisecond timescale.},
annote = {doi: 10.1162/089976604773135069},
author = {Eden, Uri T and Frank, Loren M. and Barbieri, Riccardo and Solo, Victor and Brown, Emery N.},
doi = {10.1162/089976604773135069},
issn = {0899-7667},
journal = {Neural Computation},
month = {may},
number = {5},
pages = {971--998},
publisher = {MIT Press},
title = {{Dynamic Analysis of Neural Encoding by Point Process Adaptive Filtering}},
url = {https://doi.org/10.1162/089976604773135069},
volume = {16},
year = {2004}
}
@article{Gaunt2019,
abstract = {The Conway-Maxwell-Poisson distribution is a two-parameter generalization of the Poisson distribution that can be used to model data that are under-or over-dispersed relative to the Poisson distribution. The normalizing constant Z ($\lambda$, $\nu$) is given by an infinite series that in general has no closed form, although several papers have derived approximations for this sum. In this work, we start by using probabilistic argument to obtain the leading term in the asymptotic expansion of Z ($\lambda$, $\nu$) in the limit $\lambda$ → ∞ that holds for all $\nu$ > 0. We then use an integral representation to obtain the entire asymptotic series and give explicit formulas for the first eight coefficients. We apply this asymptotic series to obtain approximations for the mean, variance, cumulants, skewness, excess kurtosis and raw moments of CMP random variables. Numerical results confirm that these correction terms yield more accurate estimates than those obtained using just the leading-order term.},
author = {Gaunt, Robert E and Iyengar, Satish and {Olde Daalhuis}, Adri B and Simsek, Burcin and {Robert Gaunt}, B E},
doi = {10.1007/s10463-017-0629-6},
journal = {Ann Inst Stat Math},
keywords = {Approximation,Asymptotic series,Conway–Maxwell–Poisson distribution,Generalized hypergeometric function,Normalizing constant,Stein's method},
pages = {163--180},
title = {{An asymptotic expansion for the normalizing constant of the Conway-Maxwell-Poisson distribution}},
url = {https://doi.org/10.1007/s10463-017-0629-6},
volume = {71},
year = {2019}
}
@article{RAUCH1965,
annote = {doi: 10.2514/3.3166},
author = {Rauch, H E and Tung, F and Striebel, C T},
doi = {10.2514/3.3166},
issn = {0001-1452},
journal = {AIAA Journal},
month = {aug},
number = {8},
pages = {1445--1450},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{Maximum likelihood estimates of linear dynamic systems}},
url = {https://doi.org/10.2514/3.3166},
volume = {3},
year = {1965}
}